{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbe13be-7242-41d8-ad5b-ad7053adba58",
   "metadata": {},
   "source": [
    "# Spark - Machine Learning Fundamentals\n",
    "In this courselet, we are going to explore the basics in the use of Spark as a tool to train a Machine Learning model. By the end of this courselet, you should be able to:\n",
    "\n",
    "- Recognize the fundamentals in training a ML model using Spark\n",
    "- Identify two different ML algorithms\n",
    "- Identify the main libraries and documentation to perform your tasks\n",
    "\n",
    "This courselet presupposes a foundational understanding of fundamental machine learning processes and the Spark framework. It is designed primarily to introduce coding within these contexts, rather than to focus exclusively on the development of rigorously accurate models.\n",
    "\n",
    "For this courselet, we will use taxi trips reported to the City of Chicago in 2020. This data is publicly available through the [Chicago Data Portal](https://data.cityofchicago.org/en/Transportation/Taxi-Trips-2020/r2u4-wwk3/about_data) If you previously covered the Exploratory Data Analysis with Spark courselet, you should be familiar with this dataset. \n",
    "\n",
    "In this courselet, we are going to explore the following cases:\n",
    "\n",
    "- **Regression:** We are going to try to estimate the fare price of a trip, given a collection of features based on location, temporality, and trip duration.\n",
    "- **Clustering:** We are going to segmentate our trips in 10 different clusters, using the coordinates and temporality components as our clustering features.\n",
    "\n",
    "We are using the data in [Parquet format](https://parquet.apache.org/), given the several advantages of this format.\n",
    "\n",
    "## Module 1: Regression\n",
    "\n",
    "As a very first step, we will start by initiating our Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ba1c0-f90a-4d67-82b4-b8bd66399ecc",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML-Process-Regression\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacff68-4171-409e-87e8-fe8004131bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading the data and printing the schema\n",
    "df = spark.read.parquet(\"data/chicago-taxi-2020.parquet\",header=True, inferSchema=True)\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59bf8a-8eb4-4068-bfc1-ce0ce588095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A display of the first few rows\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b4e2f-ce64-408c-8810-f42394b76e23",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "We briefly explore our data. We might want to start by analyzing the features that will be part of our regression model and displaying the missingness rate per column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae25566-0f9d-4861-b1d5-0c50aa11dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of continuous variables for our regression analysis\n",
    "continuous = [\"Trip Seconds\", \"Trip Miles\", \"Fare\", \"Tips\", \"Tolls\", \"Extras\", \"Trip Total\"]\n",
    "df_cont = df.select(continuous)\n",
    "df_cont_summary = df_cont.describe()\n",
    "\n",
    "df_cont_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dffcae-51d6-41d4-9f14-92e150c6256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness rate per column\n",
    "from pyspark.sql.functions import col, count, when, lit\n",
    "total_count = df.count()\n",
    "missingness_rate = df.select([((count(when(col(c).isNull(), c)) / lit(total_count))).alias(c) for c in df.columns])\n",
    "\n",
    "missingness_rate.toPandas().transpose() # Using toPandas method to make it look nicely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1983d44-d37d-4a72-a371-c5bcad44f1eb",
   "metadata": {},
   "source": [
    "### Data pre-processing and feature engineering\n",
    "\n",
    "**Regression Case**\n",
    "\n",
    "For our regression modeling, our data pre-processing will go as follows:\n",
    "1. We will start by creating a sub-df in which we'll exclusively keep those features that are part of our analysis\n",
    "2. We will remove outliers from the *Fare* column (our target feature) by using the [1.5xIQR rule](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule#:~:text=A%20commonly%20used%20rule%20says,3%20%2B%201.5%20%E2%8B%85%20IQR%20%E2%80%8D%20.)\n",
    "3. We will extract the hour and the day of the week from *Trip Start Timestamp*\n",
    "4. We will encode the hour, day of the week and community area of the pick up to treat them as categories for the model\n",
    "5. We will place all of our explanatory features into a vector column using [VectorAssembler](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)\n",
    "6. We will reduce the number of features using Principal Component Analysis\n",
    "7. We will create our final dataframe, keeping the relevant features with *Fare*, our target column.\n",
    "\n",
    "**Vector Assembler** \n",
    "\n",
    "An important component of our data preparation pipeline will be the transformation of our dataframees into a column vector representations through the use of the VectorAssembler class. By transforming our dataframe into this representation, as PySpark algorithms need data to be represented like that in order to achieve an efficient parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838304d7-982a-4429-bdab-084dcbfbc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select regression features\n",
    "regression_features = [\"Trip Start Timestamp\", \"Trip Seconds\", \"Trip Miles\", \"Pickup Community Area\", \"Fare\"]\n",
    "df_reg = df.select(*regression_features).dropna(how='any', subset=regression_features) # We make sure to drop any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf75cd-6304-4bb4-b33f-6edd89cd03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from our target column\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "quantiles = df_reg.approxQuantile(\"Fare\", [0.25, 0.75], 0.05) # https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.approxQuantile.html\n",
    "Q1, Q3 = quantiles\n",
    "\n",
    "IQR = Q3 - Q1 #Calculate IQR\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_reg = df_reg.filter((col(\"Fare\") >= lit(lower_bound)) & (col(\"Fare\") <= lit(upper_bound))) # We remove the outliers using the bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735147c-dccd-4e06-b0fe-e1aa98f50946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import to_timestamp, hour, dayofweek\n",
    "\n",
    "# We start by converting the column's format to timestamp and extracting hour and day of the week\n",
    "df_reg =  df_reg.withColumn(\"Trip Start Timestamp\", \n",
    "                            to_timestamp(\"Trip Start Timestamp\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "                .withColumn(\"pickup_hour\", hour(\"Trip Start Timestamp\")) \\\n",
    "                .withColumn(\"pickup_day_of_week\", dayofweek(\"Trip Start Timestamp\"))\n",
    "\n",
    "# We have to convert the new columns into string format to index them\n",
    "df_reg = df_reg.withColumn(\"pickup_hour\", df_reg[\"pickup_hour\"].cast(\"string\")) \\\n",
    "               .withColumn(\"pickup_day_of_week\", df_reg[\"pickup_day_of_week\"].cast(\"string\"))\n",
    "\n",
    "# Now we index and encode the new columns, along with Pickup Community Area\n",
    "hour_indexer = StringIndexer(inputCol=\"pickup_hour\", \n",
    "                             outputCol=\"pickup_hour_indexed\")\n",
    "day_of_week_indexer = StringIndexer(inputCol=\"pickup_day_of_week\", \n",
    "                                    outputCol=\"pickup_day_of_week_indexed\")\n",
    "community_area_indexer = StringIndexer(inputCol=\"Pickup Community Area\", \n",
    "                                       outputCol=\"Pickup Community Area Index\")\n",
    "\n",
    "hour_encoder = OneHotEncoder(inputCols=[\"pickup_hour_indexed\"], \n",
    "                             outputCols=[\"pickup_hour_vec\"])\n",
    "day_of_week_encoder = OneHotEncoder(inputCols=[\"pickup_day_of_week_indexed\"], \n",
    "                                    outputCols=[\"pickup_day_of_week_vec\"])\n",
    "community_area_encoder = OneHotEncoder(inputCols=[\"Pickup Community Area Index\"], \n",
    "                                       outputCols=[\"pickup_community_area_vec\"])\n",
    "\n",
    "# We create a transformations (pipeline https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html)\n",
    "pipeline_reg = Pipeline(stages=[hour_indexer, day_of_week_indexer, \n",
    "                            community_area_indexer, hour_encoder, \n",
    "                            day_of_week_encoder, community_area_encoder])\n",
    "\n",
    "# Now we execute the pipeline\n",
    "df_reg = pipeline_reg.fit(df_reg).transform(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b22812-955a-4ce9-9171-817e767beca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we transform our features of interest into a single column vector called \"features\" \n",
    "reg_input_cols = [\"Trip Seconds\", \"Trip Miles\", \"pickup_hour_vec\", \"pickup_day_of_week_vec\", \"pickup_community_area_vec\"]\n",
    "reg_assembler = VectorAssembler(inputCols=reg_input_cols, outputCol=\"features\")\n",
    "df_reg = reg_assembler.transform(df_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d629d3-feb0-4a46-861e-a115337911cc",
   "metadata": {},
   "source": [
    "Now, to reduce the dimensionality and only keep the most important features, we will apply Principal Component Analysis to reduce the number of features and only keep 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f875fe-2e24-406e-bc38-5534d97f34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# We apply PCA on the \"features\" column\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pcaModel = pca.fit(df_reg)\n",
    "df_reg_pca = pcaModel.transform(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2e002-865f-4174-8324-86c16b011ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final data only includes our components vector and the target column (Fare)\n",
    "reg_data= df_reg_pca.select(col(\"pcaFeatures\").alias(\"features\"), col(\"Fare\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4850c-91a7-41c0-b3da-fee22da84d99",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We proceed to train our model. First, we will split the data into our training and testing datasets at a 80/20 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e0cd2-71ec-4087-ae22-c89680ebc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing\n",
    "train, test = reg_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909abfc0-8e6c-4d91-9a0d-e20938742001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train the model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Fare\")\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141acc5-59ee-4ecc-9680-c27973cb10a8",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the performance of our regression model, we calculate the [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b22958-d0cd-474a-aa1a-bf26c08e7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# We first make create a new df with a new columns named \"prediction\", using our LR Model\n",
    "predictions = lr_model.transform(test)\n",
    "\n",
    "# Now we create a evaluator, setting the column, the\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"Fare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = reg_evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1700b-3bb7-456b-8631-eeef9cf4613c",
   "metadata": {},
   "source": [
    "### Saving the model for deployment\n",
    "\n",
    "Once we are comfortable with the results of a model, and we want to make it availble for production deployment, we can store the model through the *.save()* method, to eventually load it in the production environment using the *.load()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e95b9b-288e-41b9-8385-899e01d96da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the current path for storing. This can be any path\n",
    "path = \"the/path/to/lr_model\"\n",
    "lr_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a43683-7d8e-49b1-b1e8-3ffe66ef86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, in a production environment, we could load the model like this\n",
    "\n",
    "model_path = \"the/path/to/lr_model\"\n",
    "model = LinearRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ca798-142a-4a36-a629-f9ba73b0ad9e",
   "metadata": {},
   "source": [
    "### Explore More\n",
    "\n",
    "The extensive collection of algorithms, framweworks and utilities that PySpark offers for Machine Learning tasks can be found in the following links:\n",
    "\n",
    "- [MLlib (DataFrame-based)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)\n",
    "- [MLlib (RDD-based)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
