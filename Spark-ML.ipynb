{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a76281-0487-4fac-be17-2cc63745ff04",
   "metadata": {},
   "source": [
    "# Spark - Machine Learning Fundamentals\n",
    "In this courselet, we are going to explore the basics in the use of Spark as a tool to train a Machine Learning model. By the end of this courselet, you should be able to:\n",
    "\n",
    "- Recognize the fundamentals in training a ML model using Spark\n",
    "- Identify two different ML algorithms\n",
    "- Identify the main libraries and documentation to perform your tasks\n",
    "\n",
    "This courselet presupposes a foundational understanding of fundamental machine learning processes and the Spark framework. It is designed primarily to introduce coding within these contexts, rather than to focus exclusively on the development of rigorously accurate models.\n",
    "\n",
    "For this courselet, we will use taxi trips reported to the City of Chicago in 2020. This data is publicly available through the [Chicago Data Portal](https://data.cityofchicago.org/en/Transportation/Taxi-Trips-2020/r2u4-wwk3/about_data) If you previously covered the Exploratory Data Analysis with Spark courselet, you should be familiar with this dataset. \n",
    "\n",
    "In this courselet, we are going to explore the following cases:\n",
    "\n",
    "- **Regression:** We are going to try to estimate the fare price of a trip, given a collection of features based on location, temporality, and trip duration.\n",
    "- **Clustering:** We are going to segmentate our trips in 10 different clusters, using the coordinates and temporality components as our clustering features.\n",
    "\n",
    "We are using the data in [Parquet format](https://parquet.apache.org/), given the several advantages of this format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a604e-ea6d-4375-9c31-e92c0f321aa2",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "As a very first step, we will start by initiating our Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50266b00-ab4c-4684-8be0-d5258453d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML Process\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45739728-6ec0-4a8b-a35f-cf0c69fd5a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: string (nullable = true)\n",
      " |-- Taxi ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: double (nullable = true)\n",
      " |-- Trip Miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: double (nullable = true)\n",
      " |-- Dropoff Census Tract: double (nullable = true)\n",
      " |-- Pickup Community Area: double (nullable = true)\n",
      " |-- Dropoff Community Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tips: double (nullable = true)\n",
      " |-- Tolls: double (nullable = true)\n",
      " |-- Extras: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Payment Type: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Pickup Centroid Latitude: double (nullable = true)\n",
      " |-- Pickup Centroid Longitude: double (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: double (nullable = true)\n",
      " |-- Dropoff Centroid  Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We start by loading the data and printing the schema\n",
    "df = spark.read.parquet(\"data/chicago-taxi-2020.parquet\",header=True, inferSchema=True)\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d291cee-7170-4a51-b3c0-19909a980a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Taxi ID</th>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <th>Trip Seconds</th>\n",
       "      <th>Trip Miles</th>\n",
       "      <th>Pickup Census Tract</th>\n",
       "      <th>Dropoff Census Tract</th>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <th>...</th>\n",
       "      <th>Extras</th>\n",
       "      <th>Trip Total</th>\n",
       "      <th>Payment Type</th>\n",
       "      <th>Company</th>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <th>Dropoff Centroid  Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16c7456d99031528c238bd02f40df5ab9bdf9778</td>\n",
       "      <td>88d3be8c1334607f62a8c058f680dd7fbb57826ec7408d...</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.27</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>472eef1d5c7a5e5ee033f673942f367dc71869f1</td>\n",
       "      <td>199fa05b63204aa1c620c161c5cebe43b0909b1ee99864...</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>01/01/2020 12:30:00 AM</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Blue Ribbon Taxi Association Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>41.761578</td>\n",
       "      <td>-87.572782</td>\n",
       "      <td>POINT (-87.5727819867 41.7615779081)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>031a4d882fb3315a490d0b5c358c945ad9b9856d</td>\n",
       "      <td>aabecb47e958f99860a3b4d01f14d53644ac26126d9519...</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>01/01/2020 12:15:00 AM</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.80</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c416c246829dfb3f78cc93cbc7dfecdd379be15</td>\n",
       "      <td>ba106251fbb2b52177138ccbb8a1327a83c89470c240b2...</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>01/01/2020 12:15:00 AM</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3c0a22971ae070ce35e0c03f1b8e92fe7aa840cd</td>\n",
       "      <td>1f1970d8e52c7e2575aeb68eb3b6ab0e21c77b728267df...</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>01/01/2020 12:00:00 AM</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Sun Taxi</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Trip ID  \\\n",
       "0  16c7456d99031528c238bd02f40df5ab9bdf9778   \n",
       "1  472eef1d5c7a5e5ee033f673942f367dc71869f1   \n",
       "2  031a4d882fb3315a490d0b5c358c945ad9b9856d   \n",
       "3  3c416c246829dfb3f78cc93cbc7dfecdd379be15   \n",
       "4  3c0a22971ae070ce35e0c03f1b8e92fe7aa840cd   \n",
       "\n",
       "                                             Taxi ID    Trip Start Timestamp  \\\n",
       "0  88d3be8c1334607f62a8c058f680dd7fbb57826ec7408d...  01/01/2020 12:00:00 AM   \n",
       "1  199fa05b63204aa1c620c161c5cebe43b0909b1ee99864...  01/01/2020 12:00:00 AM   \n",
       "2  aabecb47e958f99860a3b4d01f14d53644ac26126d9519...  01/01/2020 12:00:00 AM   \n",
       "3  ba106251fbb2b52177138ccbb8a1327a83c89470c240b2...  01/01/2020 12:00:00 AM   \n",
       "4  1f1970d8e52c7e2575aeb68eb3b6ab0e21c77b728267df...  01/01/2020 12:00:00 AM   \n",
       "\n",
       "       Trip End Timestamp  Trip Seconds  Trip Miles  Pickup Census Tract  \\\n",
       "0  01/01/2020 12:00:00 AM          60.0        0.00                  NaN   \n",
       "1  01/01/2020 12:30:00 AM        1740.0        0.00                  NaN   \n",
       "2  01/01/2020 12:15:00 AM         720.0        0.70                  NaN   \n",
       "3  01/01/2020 12:15:00 AM         720.0        0.80                  NaN   \n",
       "4  01/01/2020 12:00:00 AM         556.0        0.77                  NaN   \n",
       "\n",
       "   Dropoff Census Tract  Pickup Community Area  Dropoff Community Area  ...  \\\n",
       "0                   NaN                    NaN                     NaN  ...   \n",
       "1                   NaN                    NaN                    43.0  ...   \n",
       "2                   NaN                    8.0                     8.0  ...   \n",
       "3                   NaN                    8.0                     8.0  ...   \n",
       "4                   NaN                    8.0                     8.0  ...   \n",
       "\n",
       "   Extras  Trip Total  Payment Type                            Company  \\\n",
       "0     9.0       12.27   Credit Card                Top Cab Affiliation   \n",
       "1     0.0       13.00          Cash  Blue Ribbon Taxi Association Inc.   \n",
       "2     5.0       14.80   Credit Card          Taxi Affiliation Services   \n",
       "3     2.0        9.75          Cash          Taxi Affiliation Services   \n",
       "4     0.0        6.75          Cash                           Sun Taxi   \n",
       "\n",
       "   Pickup Centroid Latitude Pickup Centroid Longitude  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                 41.899602                -87.633308   \n",
       "3                 41.899602                -87.633308   \n",
       "4                 41.899602                -87.633308   \n",
       "\n",
       "              Pickup Centroid Location  Dropoff Centroid Latitude  \\\n",
       "0                                 None                        NaN   \n",
       "1                                 None                  41.761578   \n",
       "2  POINT (-87.6333080367 41.899602111)                  41.899602   \n",
       "3  POINT (-87.6333080367 41.899602111)                  41.899602   \n",
       "4  POINT (-87.6333080367 41.899602111)                  41.899602   \n",
       "\n",
       "   Dropoff Centroid Longitude            Dropoff Centroid  Location  \n",
       "0                         NaN                                  None  \n",
       "1                  -87.572782  POINT (-87.5727819867 41.7615779081)  \n",
       "2                  -87.633308   POINT (-87.6333080367 41.899602111)  \n",
       "3                  -87.633308   POINT (-87.6333080367 41.899602111)  \n",
       "4                  -87.633308   POINT (-87.6333080367 41.899602111)  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A display of the first few rows\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc5ffd-40fa-4e9f-a873-79380ba54e73",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "We briefly explore our data. We might want to start by analyzing the features that will be part of our regression model and displaying the missingness rate per column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3212a7b1-9c34-4517-8968-576df24b25ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+--------------------+------------------+-----------------+\n",
      "|summary|      Trip Seconds|       Trip Miles|              Fare|              Tips|               Tolls|            Extras|       Trip Total|\n",
      "+-------+------------------+-----------------+------------------+------------------+--------------------+------------------+-----------------+\n",
      "|  count|           3887483|          3889002|           3888700|           3888700|             3888700|           3888700|          3888700|\n",
      "|   mean| 874.7527292080763|3.677313007297242|15.701397827037205|1.4626049785276904|0.002017751433641...|1.0959525856970214|18.36848553758443|\n",
      "| stddev|1784.5672882912645|6.099401337576302| 88.83487861203875|2.8588320586587246|  0.2049892615858179|23.893356793019223|92.44467251419816|\n",
      "|    min|               0.0|              0.0|               0.0|               0.0|                 0.0|               0.0|              0.0|\n",
      "|    max|           86398.0|            993.6|           9955.55|             512.0|               100.0|           9900.66|          9955.55|\n",
      "+-------+------------------+-----------------+------------------+------------------+--------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of continuous variables for our regression analysis\n",
    "continuous = [\"Trip Seconds\", \"Trip Miles\", \"Fare\", \"Tips\", \"Tolls\", \"Extras\", \"Trip Total\"]\n",
    "df_cont = df.select(continuous)\n",
    "df_cont_summary = df_cont.describe()\n",
    "\n",
    "df_cont_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75227f91-8ecb-4485-a495-937387ca77c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Taxi ID</th>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <th>Trip Seconds</th>\n",
       "      <th>Trip Miles</th>\n",
       "      <th>Pickup Census Tract</th>\n",
       "      <th>Dropoff Census Tract</th>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <th>...</th>\n",
       "      <th>Extras</th>\n",
       "      <th>Trip Total</th>\n",
       "      <th>Payment Type</th>\n",
       "      <th>Company</th>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <th>Dropoff Centroid  Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.541555</td>\n",
       "      <td>0.544762</td>\n",
       "      <td>0.071719</td>\n",
       "      <td>0.094838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071538</td>\n",
       "      <td>0.071538</td>\n",
       "      <td>0.071538</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.093113</td>\n",
       "      <td>0.093113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trip ID   Taxi ID  Trip Start Timestamp  Trip End Timestamp  Trip Seconds  \\\n",
       "0      0.0  0.000052                   0.0            0.000148      0.000398   \n",
       "\n",
       "   Trip Miles  Pickup Census Tract  Dropoff Census Tract  \\\n",
       "0    0.000008             0.541555              0.544762   \n",
       "\n",
       "   Pickup Community Area  Dropoff Community Area  ...    Extras  Trip Total  \\\n",
       "0               0.071719                0.094838  ...  0.000085    0.000085   \n",
       "\n",
       "   Payment Type  Company  Pickup Centroid Latitude  Pickup Centroid Longitude  \\\n",
       "0           0.0      0.0                  0.071538                   0.071538   \n",
       "\n",
       "   Pickup Centroid Location  Dropoff Centroid Latitude  \\\n",
       "0                  0.071538                   0.093113   \n",
       "\n",
       "   Dropoff Centroid Longitude  Dropoff Centroid  Location  \n",
       "0                    0.093113                    0.093113  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missingness rate per column\n",
    "from pyspark.sql.functions import col, count, when, lit\n",
    "total_count = df.count()\n",
    "missingness_rate = df.select([((count(when(col(c).isNull(), c)) / lit(total_count))).alias(c) for c in df.columns])\n",
    "\n",
    "missingness_rate.toPandas() # Using toPandas method to make it look nicely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac25f11-29ad-4714-82b0-09d1e52d60df",
   "metadata": {},
   "source": [
    "### Data pre-processing and feature engineering\n",
    "\n",
    "**Regression Case**\n",
    "\n",
    "For our regression modeling, our data pre-processing will go as follows:\n",
    "1. We will start by creating a sub-df in which we'll exclusively keep those features that are part of our analysis\n",
    "2. We will remove outliers from the *Fare* column (our target feature) by using the [1.5xIQR rule](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule#:~:text=A%20commonly%20used%20rule%20says,3%20%2B%201.5%20%E2%8B%85%20IQR%20%E2%80%8D%20.)\n",
    "3. We will extract the hour and the day of the week from *Trip Start Timestamp*\n",
    "4. We will encode the hour, day of the week and community area of the pick up to treat them as categories for the model\n",
    "5. We will place all of our explanatory features into a vector column using [VectorAssembler](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)\n",
    "6. We will reduce the number of features using Principal Component Analysis\n",
    "7. We will create our final dataframe, keeping the relevant features with *Fare*, our target column.\n",
    "\n",
    "**Vector Assembler** \n",
    "\n",
    "An important component of our data preparation pipeline will be the transformation of our dataframees into a column vector representations through the use of the VectorAssembler class. By transforming our dataframe into this representation, as PySpark algorithms need data to be represented like that in order to achieve an efficiente parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23275376-d0b1-43e5-865f-535176067b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select regression features\n",
    "regression_features = [\"Trip Start Timestamp\", \"Trip Seconds\", \"Trip Miles\", \"Pickup Community Area\", \"Fare\"]\n",
    "df_reg = df.select(*regression_features).dropna(how='any', subset=regression_features) # We make sure to drop any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2fa21f-6157-4c8d-b896-3f638145db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from our target column\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "quantiles = df_reg.approxQuantile(\"Fare\", [0.25, 0.75], 0.05) # https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.approxQuantile.html\n",
    "Q1, Q3 = quantiles\n",
    "\n",
    "IQR = Q3 - Q1 #Calculate IQR\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_reg = df_reg.filter((col(\"Fare\") >= lit(lower_bound)) & (col(\"Fare\") <= lit(upper_bound))) # We remove the outliers using the bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b732c84c-2d9b-492d-8c64-d3f6ca0afddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import to_timestamp, hour, dayofweek\n",
    "\n",
    "# We start by converting the column's format to timestamp and extracting hour and day of the week\n",
    "df_reg =  df_reg.withColumn(\"Trip Start Timestamp\", \n",
    "                            to_timestamp(\"Trip Start Timestamp\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "                .withColumn(\"pickup_hour\", hour(\"Trip Start Timestamp\")) \\\n",
    "                .withColumn(\"pickup_day_of_week\", dayofweek(\"Trip Start Timestamp\"))\n",
    "\n",
    "# We have to convert the new columns into string format to index them\n",
    "df_reg = df_reg.withColumn(\"pickup_hour\", df_reg[\"pickup_hour\"].cast(\"string\")) \\\n",
    "               .withColumn(\"pickup_day_of_week\", df_reg[\"pickup_day_of_week\"].cast(\"string\"))\n",
    "\n",
    "# Now we index and encode the new columns, along with Pickup Community Area\n",
    "hour_indexer = StringIndexer(inputCol=\"pickup_hour\", \n",
    "                             outputCol=\"pickup_hour_indexed\")\n",
    "day_of_week_indexer = StringIndexer(inputCol=\"pickup_day_of_week\", \n",
    "                                    outputCol=\"pickup_day_of_week_indexed\")\n",
    "community_area_indexer = StringIndexer(inputCol=\"Pickup Community Area\", \n",
    "                                       outputCol=\"Pickup Community Area Index\")\n",
    "\n",
    "hour_encoder = OneHotEncoder(inputCols=[\"pickup_hour_indexed\"], \n",
    "                             outputCols=[\"pickup_hour_vec\"])\n",
    "day_of_week_encoder = OneHotEncoder(inputCols=[\"pickup_day_of_week_indexed\"], \n",
    "                                    outputCols=[\"pickup_day_of_week_vec\"])\n",
    "community_area_encoder = OneHotEncoder(inputCols=[\"Pickup Community Area Index\"], \n",
    "                                       outputCols=[\"pickup_community_area_vec\"])\n",
    "\n",
    "# We create a transformations (pipeline https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html)\n",
    "pipeline_reg = Pipeline(stages=[hour_indexer, day_of_week_indexer, \n",
    "                            community_area_indexer, hour_encoder, \n",
    "                            day_of_week_encoder, community_area_encoder])\n",
    "\n",
    "# Now we execute the pipeline\n",
    "df_reg = pipeline_reg.fit(df_reg).transform(df_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f17a65-cc4b-4baa-9179-071c3b317623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we transform our features of interest into a single column vector called \"features\" \n",
    "reg_input_cols = [\"Trip Seconds\", \"Trip Miles\", \"pickup_hour_vec\", \"pickup_day_of_week_vec\", \"pickup_community_area_vec\"]\n",
    "reg_assembler = VectorAssembler(inputCols=reg_input_cols, outputCol=\"features\")\n",
    "df_reg = reg_assembler.transform(df_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b97b32-fe02-4975-a956-ad2c5d6d16fa",
   "metadata": {},
   "source": [
    "Now, to reduce the dimensionality and only keep the most important features, we will apply [Principal Component Analysis]() to reduce the number of features and only keep 3 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e7fabe-8c9d-4d56-a7ff-da3ffca6391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# We apply PCA on the \"features\" column\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pcaModel = pca.fit(df_reg)\n",
    "df_reg_pca = pcaModel.transform(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c1b1506-3ed0-4e53-946a-9c85fca4003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final data only includes our components vector and the target column (Fare)\n",
    "reg_data= df_reg_pca.select(col(\"pcaFeatures\").alias(\"features\"), col(\"Fare\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cf6a1-ad75-43d2-bdb0-28f822b85398",
   "metadata": {},
   "source": [
    "**Clustering Case**\n",
    "\n",
    "For our regression modeling, our data pre-processing will go as follows:\n",
    "\n",
    "1. We will start by creating a sub-df in which we'll exclusively keep those features that are part of our analysis\n",
    "2. We will extract the hour and the day of the week from *Trip Start Timestamp*\n",
    "3. We will encode the hour and day of the week to treat them as categories\n",
    "4. We will place all of our explanatory features into a vector column using [VectorAssembler](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html)l data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2d4b4d-7c75-4f74-aea7-f6e2ae2164f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select clustering features \n",
    "clustering_features = [\"Trip Start Timestamp\", \"Pickup Centroid Latitude\", \"Pickup Centroid Longitude\"]\n",
    "df_clust = df.select(*clustering_features).dropna(how='any', subset=clustering_features) # We make sure to drop any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7254e418-1098-4a78-8131-d5f6f6295d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines represent a similar process as the one we did before for regression\n",
    "df_clust =  df_clust.withColumn(\"Trip Start Timestamp\", \n",
    "                            to_timestamp(\"Trip Start Timestamp\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "                .withColumn(\"pickup_hour\", hour(\"Trip Start Timestamp\")) \\\n",
    "                .withColumn(\"pickup_day_of_week\", dayofweek(\"Trip Start Timestamp\"))\n",
    "\n",
    "df_clust = df_clust.withColumn(\"pickup_hour\", df_clust[\"pickup_hour\"].cast(\"string\")) \\\n",
    "               .withColumn(\"pickup_day_of_week\", df_clust[\"pickup_day_of_week\"].cast(\"string\"))\n",
    "\n",
    "pipeline_clust = Pipeline(stages=[hour_indexer, day_of_week_indexer, \n",
    "                            hour_encoder, day_of_week_encoder])\n",
    "\n",
    "df_clust = pipeline_clust.fit(df_clust).transform(df_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ee1ca7-c1be-4207-ba87-bd4b162988af",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_input_cols = [\"pickup_hour_vec\", \"pickup_day_of_week_vec\", \"Pickup Centroid Latitude\", \"Pickup Centroid Longitude\"]\n",
    "clust_assembler = VectorAssembler(inputCols=clust_input_cols, outputCol=\"features\")\n",
    "df_clust = clust_assembler.transform(df_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437314b-a5c0-4a88-8770-3a75a4b67f8c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd62683-d989-48f0-a197-55f3f8398123",
   "metadata": {},
   "source": [
    "**Regression**\n",
    "\n",
    "We proceed to train our model. First, we will split the data into our training and testing datasets at a 80/20 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c57dc3e-b35b-4060-92ed-3189ffd681ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into training and testing\n",
    "train, test = reg_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd991a0-1fd6-49fb-93c7-bbf0b246346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train the model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Fare\")\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736dd19-4ed8-4da3-9ce8-307256066a14",
   "metadata": {},
   "source": [
    "**Clustering**\n",
    "\n",
    "Given the nature of cluster analysis, splitting is not necessary. We set the number of clusters to 10 and run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801f94cf-5c8c-4b1c-b3f2-166ceb968949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(10).setSeed(123)\n",
    "cluster_model = kmeans.fit(df_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833f4c6-e5aa-468a-b7a9-ec9a9b4ddbd8",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "**Regression** \n",
    "\n",
    "To evaluate the performance of our regression model, we calculate the [Root Mean Square Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db05101c-5bdd-4e0b-b60c-b7ef23f020ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.272231860406048\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# We first make create a new df with a new columns named \"prediction\", using our LR Model\n",
    "predictions = lr_model.transform(test)\n",
    "\n",
    "# Now we create a evaluator, setting the column, the\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"Fare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = reg_evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc720a66-ff47-489b-bf79-66d2301120a8",
   "metadata": {},
   "source": [
    "**Clustering**\n",
    "\n",
    "For clustering, we are evaluating by using the [Silhouette Score](https://en.wikipedia.org/wiki/Silhouette_(clustering)#:~:text=The%20silhouette%20ranges%20from%20%E2%88%921,poorly%20matched%20to%20neighboring%20clusters.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "740db47b-2fb4-4981-af08-84fa9ed2ab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.3096852624096427\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "clusters = cluster_model.transform(df_clust)\n",
    "\n",
    "# Evaluate the model\n",
    "clust_evaluator = ClusteringEvaluator()\n",
    "\n",
    "# Calculate Silhoutte Score\n",
    "silhouette = clust_evaluator.evaluate(clusters)\n",
    "print(f\"Silhouette Score: {silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62218076-9abe-4fd7-ac10-ca877e32e5d9",
   "metadata": {},
   "source": [
    "### Saving the model for deployment\n",
    "\n",
    "Once we are comfortable with the results of a model, and we want to make it availble for production deployment, we can store the model through the *.save()* method, to eventually load it in the production environment using the *.load()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0500e8-58a7-46f3-9f30-00f2e3130205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the current path for storing. This can be any path\n",
    "path = \"the/path/to/lr_model\"\n",
    "lr_model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc7093-5e7c-40bd-a0f0-3ede0f2c46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, in a production environment, we could load the model like this\n",
    "\n",
    "model_path = \"the/path/to/lr_model\"\n",
    "model = LinearRegressionModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0eaec9-2509-4214-bc3b-2e2c87bf5dc7",
   "metadata": {},
   "source": [
    "### Explore More\n",
    "\n",
    "The extensive collection of algorithms, framweworks and utilities that PySpark offers for Machine Learning tasks can be found in the following links:\n",
    "\n",
    "- [MLlib (DataFrame-based)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)\n",
    "- [MLlib (RDD-based)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
